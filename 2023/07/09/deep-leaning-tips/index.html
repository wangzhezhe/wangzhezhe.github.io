<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Deep leaning notes | AverageMind</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Some notes about the deep learning related knowledge.">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep leaning notes">
<meta property="og:url" content="http://yoursite.com/2023/07/09/deep-leaning-tips/index.html">
<meta property="og:site_name" content="AverageMind">
<meta property="og:description" content="Some notes about the deep learning related knowledge.">
<meta property="og:locale">
<meta property="article:published_time" content="2023-07-09T14:47:55.000Z">
<meta property="article:modified_time" content="2024-05-17T00:15:13.875Z">
<meta property="article:author" content="zhe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="AverageMind" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  
<link rel="stylesheet" href="/css/style.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-165927341-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
<!-- Google adds -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5642436380582343"
     crossorigin="anonymous">
</script>
<!-- End Google adds -->




<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">AverageMind</a>
      </h1>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-deep-leaning-tips" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/09/deep-leaning-tips/" class="article-date">
  <time datetime="2023-07-09T14:47:55.000Z" itemprop="datePublished">2023-07-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/AI-Math-NumericalAnalysis/">AI&Math&NumericalAnalysis</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Deep leaning notes
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>Some notes about the deep learning related knowledge.</p>
<span id="more"></span>

<p>There is a strong feeling that if you do not learn and use learning things actively, you will be eliminated quickly, failed to apply fundings and hard to find a good job.</p>
<p>instead of considering why I need to learn this and how it can be used into my own work, a practical attitude might be to learn it firstly.</p>
<p>The main index are this series of youtube <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=YCzL96nL7j0&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=16">video</a></p>
<h3 id="Things-need-to-consider-before-using-NN"><a href="#Things-need-to-consider-before-using-NN" class="headerlink" title="Things need to consider before using NN"></a>Things need to consider before using NN</h3><p>1 There are large amount of the data set, the NN model is data hungary</p>
<p>2 The mechanism is complicated, it is hard to figure out things by just analysing the problem. If all situation of the problem is clear, we can use other simpler model to solve the problem</p>
<p>3 Do not waste a lot of time in figureing out details of the pytorch api (there are multiple ways to do similar things), but we should at least understand the typical API. the important thing is to formulate the problem, considering input&#x2F;output, networking structures. </p>
<h3 id="Types-of-the-problem"><a href="#Types-of-the-problem" class="headerlink" title="Types of the problem"></a>Types of the problem</h3><p>Classification problem (if the new target is a cat or a dog) and the regression problem (What is the house price for a new situation).</p>
<p><strong>Regression using dl</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vyX2_XRHO9g">https://www.youtube.com/watch?v=vyX2_XRHO9g</a></p>
<p>Regression based on cnn<br><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2019/01/28/keras-regression-and-cnns/">https://pyimagesearch.com/2019/01/28/keras-regression-and-cnns/</a></p>
<p>Train NN to predict the rotation angle of the image<br><a target="_blank" rel="noopener" href="https://www.mathworks.com/help/deeplearning/ug/train-a-convolutional-neural-network-for-regression.html">https://www.mathworks.com/help/deeplearning/ug/train-a-convolutional-neural-network-for-regression.html</a></p>
<h3 id="Format-of-the-data"><a href="#Format-of-the-data" class="headerlink" title="Format of the data"></a>Format of the data</h3><p>Table data (multiple attribute, one label or numerical output)</p>
<p>Image data (one image, one label or numerical output)</p>
<p>More complicated data</p>
<h3 id="Linear-transformation-and-non-linear-transformation"><a href="#Linear-transformation-and-non-linear-transformation" class="headerlink" title="Linear transformation and non-linear transformation"></a>Linear transformation and non-linear transformation</h3><p>In the linear algebra, we use a lot about the matrix operation, they can be viewed as the scale and translation transformation of the base vectors (linear <a target="_blank" rel="noopener" href="https://www.andreinc.net/2021/01/20/eigenvalues-and-eigenvectors-explained#linear-transformations">transformation</a>). However, in the deep learning field, there are a lot of non-linear transformation, so there are more various operations and associated functions, such as ReLU. This might be help to do some complex pattern recognition. </p>
<h3 id="Typical-NN"><a href="#Typical-NN" class="headerlink" title="Typical NN"></a>Typical NN</h3><p>This blog provide a good representation with different colors (<a target="_blank" rel="noopener" href="https://www.jeremyjordan.me/intro-to-neural-networks/">https://www.jeremyjordan.me/intro-to-neural-networks/</a>).</p>
<p>We do not go through the most basic concepts about neuro networks, just put a simple form of mathmetical expression here. Assuming there are 4 input and 3 output, it is ensiensially a matrix operation. The transformation matrix should be 3 by 4. The row index represents this weigt belongs to which output, the colum index match with the input value. It represents how the input values are contributes to each output.</p>
<p>$$<br>\begin{pmatrix}<br>w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14} \\w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24} \\w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34}<br>\end{pmatrix}<br>\begin{pmatrix}<br>x_{1} \\x_{2} \\x_{3} \\x_{4}<br>\end{pmatrix}<br>+<br>\begin{pmatrix}<br>b_{1} \\b_{2} \\b_{3}<br>\end{pmatrix}<br>&#x3D;<br>\begin{pmatrix}<br>z_{1} \\z_{2} \\z_{3}<br>\end{pmatrix}<br>,<br>\begin{pmatrix}<br>w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14} \\w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24} \\w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34}<br>\end{pmatrix}<br>&#x3D;<br>\begin{pmatrix}<br>w_{1}^T \\w_{2}^T \\w_{3}^T<br>\end{pmatrix}<br>$$</p>
<p>The vector b represents the bias, and the value in the weight matrix represents the weight value. This is the most foundation operation. If there are multiple hidden layer, we just use z vector as the input to execute similar matrix multiplication and add new bias vector.</p>
<p>Another important operation is the activation function, it is a function that applied to each value in the vector. For example, in the previous example, after we go through the first layer, then we can apply the activavtion function to the elements in vector, the typical one is ReLU function. So each value in the z vector becomes ReLU(z) after this layer. ReLU function is a kind of activation function. (For example, relu(x) &#x3D; max (0,x), its an nonlinear transformation, when input larger than threshold, return input, otherwise, return zero)</p>
<p>The output layer usually generate one decision. The ArgMax or SoftMax is applied to a vector to interpret the results. Both the input and output for ArgMax and SoftMax is a vector. This is a good video to visualize the SoftMax results (<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ytbYRIN0N4g">https://www.youtube.com/watch?v=ytbYRIN0N4g</a>)(this video is really cool and there are a lot of visual results for math things) the idea is to transfer the output into the probability values.</p>
<p>When saying the typical NN graph, it might be helpful to understand it from mathematical’s perspective. It is important to consider what are input and output for each operator.</p>
<h4 id="Gradient-descent-appraoch"><a href="#Gradient-descent-appraoch" class="headerlink" title="Gradient descent appraoch"></a>Gradient descent appraoch</h4><p>The core motivation for the gradient descent is to transfer the learning problem into an optimization problem. There are all kinds of online tutorials regarding the gradient descent approach, the idea is that we find the minimal value of a function by iteratively way. Figuring out the details of the gradient descent is helpful to understand backward propagation.</p>
<p>(1) list the function we need to optimize, what is input and what it output<br>(2) compute the gradient of the input, which can be a vector with multiple values<br>(3) set the first start point of input, set the learning rate, and compute the next input value of the input<br>(4) using new input to compute the output value and if the output value start to converge (the difference is less than a threshold) or the we get to the maximal iteration numner, then stop the iteration, otherwise, we continue to do the iteration. </p>
<h4 id="Backward-propagation"><a href="#Backward-propagation" class="headerlink" title="Backward propagation"></a>Backward propagation</h4><p>It is straightforward to undersdanding how we map the input to output with known weight matrix in each layer. Basically, using the input to compute the output (forward propagation). In actual example, we need to compute or train the weight matrix by knowning input and output, this requires the loss function and back proporgation. Simply speaking, the loss function is just the <strong>error between the predicted resuts and the acutal results</strong></p>
<p>A loss function can be used to define the difference between the acurate value and the predicted value. Typical loss are MSR, mean squared error. The cross entropy is another type of common loss function for classification task. No matter what form of loss function we used, we can  measure the difference bewteen the predicted value and the acurate value. The goal is to minimize this loss function.</p>
<p>Further more, we want to minimize this error or loss function. The general appraoch to do this minimization is typical gradient descent approach.</p>
<p>This <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=khUVIZ3MON8&t=321s">video</a> is inspiring, which provides a really good explanation about the backward propagation. This <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=khUVIZ3MON8">video</a> provides a one input one weight case for backward propagation. Assuming the NN is simple and there is one layer one input <code>x</code>(known parameter), the output is $\hat{y}$, the accurate value is <code>y</code>(known parameter). The loss function is simple:<br>$$<br>L &#x3D; (\widehat y -y)^2 &#x3D; (w \cdot x-y)^2<br>$$<br>We want to find the gradient of the loss function towards the w, which is the unknown parameter. Based on the chain rule and gradient descent appraoch listed above, we first compute out the gradient equation:</p>
<p>$$<br>\nabla L(w) &#x3D; \frac {\partial L}{\partial w} &#x3D; \frac{\partial L}{\partial \widehat{y}} \cdot \frac{\partial \widehat{y}}{\partial w} &#x3D; 2(\widehat{y}-y) \cdot x &#x3D; 2x(w \cdot  x - y)<br>$$<br>Then assume the initial value of the w is $w^0$ we try to get the w value for the next iteration based on the learning rate <code>r</code>:</p>
<p>$$<br>w^1 &#x3D; w^0 - r \cdot \nabla L(w^0) &#x3D; w^0 - r \cdot 2x\cdot(w^0 \cdot  x - y)<br>$$<br>Then we can get a new predicted value $\widehat{y}^1 &#x3D; w^1 \cdot  x$ and new value of loss function value $L^1 &#x3D; (\widehat{y}^1-y)^2$ When the difference between $L^i$ and $L^{(i-1)}$ is less than a threshold or we get to the maximal iteration number, we can say that we find a good w value.</p>
<p>This is probably the simplest case for backward proporgation, we can either extend the number of layer or the number of input&#x2F;output at each layer to make the equation become more compicated, but ideas are same.</p>
<p>Even for the gradient descent approach, it is the most basic optimizer, there are all kinds of optimizer <a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/">options</a> when solving deep learning related approaches.</p>
<p>This <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=-zI1bldB8to">video</a> provides a good example from scratch about the backward propagation. </p>
<h4 id="The-defination-about-epoch-and-batch"><a href="#The-defination-about-epoch-and-batch" class="headerlink" title="The defination about epoch and batch"></a>The defination about epoch and batch</h4><p>Be careful about the concept of epoch: Assuming we have 2 data samples. We use sample 1 to do forward propagation, get predicted results, using gradient decent and chain rule to do the backward propagation and update waits. Then we send sample 2’s data into the updated waites and do the forward propagation and backward propagation again. We finish one epoch after all of these operations (we executed forward propagration and backward propagation for all data samples). We need many epoch to train a good neural network. According to this <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=K20lVDVjPn4">vedio</a>, in summary, the one forward pass and one backward pass for <strong>all the traning sample</strong> is called one epoch.</p>
<p>Epoch and batch are two common parameter. In one epoch, if the number of sample is too large, we might not fit all samples to device at once, we could divide them in small partitions, each partition is called one batch and size of it is the batch size. If batch size is 64, it means we use 64 samples at one time to do the forward and backward pass. So <strong>number of traning samples in one forward and backward pass is the batch size</strong> we can use the capability of GPU to accelarate the training process by feeding enough data to the device.</p>
<p>The total number of iteration in network training is number of epoch times the number of batch. If we only have one epoch and 1000 samples, assuming batch size is 500, we need two iterations to do the worards pass and backward pass. The reason that we set the batch size is larger than 1 is that the computer can be powerful to process multiple data sample in parallel to do the forward and backward pass. Larger batch size can improve the train speed, but too large batch size is not good, there is a tradeoff here.</p>
<h4 id="Using-dynamic-programming-to-find-the-gradient"><a href="#Using-dynamic-programming-to-find-the-gradient" class="headerlink" title="Using dynamic programming to find the gradient"></a>Using dynamic programming to find the gradient</h4><p>There is m to n relationship between the input and output in real NN, and there are even multiple layers. When we compute the gradient, we might need to compute the Jacobbian matrix, look at this <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf">slides</a> to get more detailed info. The general idea is really similar to way of thinking problems through dynamic programming way, we want to compute $\frac {\partial L_i}{\partial w_j}$ we know which related nodes contribute to this value in the previous layer, and we can build a graph to show this, and we can compute associated value at the first layer easily.</p>
<h4 id="Activation-function"><a href="#Activation-function" class="headerlink" title="Activation function"></a>Activation function</h4><p>Typical activation function is ReLU, which introduce some non-linear property to the network and make it to process more complicated question. The non-linear property of active function does not mean there is no derivative of it. We can still compute the derivative of it in the process of backpropagation, checking <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/42042561/relu-derivative-in-backpropagation">this question</a>.</p>
<h4 id="An-simple-pytorch-exmaple"><a href="#An-simple-pytorch-exmaple" class="headerlink" title="An simple pytorch exmaple"></a>An simple pytorch exmaple</h4><p>It is easier to look at associated simple python code after figureing out these details.</p>
<p>Let’s look at the example shown in <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=FHdlXe1bSe4&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=21">this video</a> </p>
<p>Before using pytorch to program it, it is good to try to express it clearly mathmatically. we use $w1$ and $w2$ to express two layers. For the first layer there is:<br>$$<br>\begin{pmatrix}<br>W1_{00} \\W1_{10}<br>\end{pmatrix}<br>\cdot I<br>+<br>\begin{pmatrix}<br>b1_{0} \\b1_{1}<br>\end{pmatrix}<br>&#x3D;<br>\begin{pmatrix}<br>Z1_0 \\Z1_1<br>\end{pmatrix}<br>$$<br>the first subscript represent the index of the output, there is one input value so there is one column for the matrix. Then there is a ReLU function for output and the output, it can be described as<br>$$<br>(W2_{00},W2_{01}) \cdot<br>\begin{pmatrix}<br>ReLu(Z1_{0}) \\ ReLu(Z1_{1})<br>\end{pmatrix} &#x3D; Z2<br>$$<br>which get the final output.</p>
<p>After figureing out these details, it is easy to understand associated pytorch example.</p>
<p>Associated code example can be founud <a target="_blank" rel="noopener" href="https://github.com/wangzhezhe/5MCST/blob/master/NN/BasicNN.py">here</a>, what we need to do is to provide a forward function, and the pytorch library will help us to execute the backward function and the traning function automatically. Becareful about the properties of the function such as <code>requires_gradient=True</code> (the parameter that needs to be optimized)</p>
<p>Several standard model for deep learning program in pytorch: 1 data loader 2 description of network 3 loss function 4 optimizer 5 train (descripbe the high level of whole process)</p>
<h4 id="ArgMax-and-SoftMax"><a href="#ArgMax-and-SoftMax" class="headerlink" title="ArgMax and SoftMax"></a>ArgMax and SoftMax</h4><p>These function are used to process the output of the NN. The argmax interprets the largest positive output value as 1 and all other values as zero. The softmax use a specific function to map the results into 0 and 1, there is probability for each output. All smaller values are also preserved</p>
<h4 id="Gradient-decent"><a href="#Gradient-decent" class="headerlink" title="Gradient decent"></a>Gradient decent</h4><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p>Convolutional neural network is used a lot for image related task, such as the image classification.</p>
<p>Why convolutional operation is helpful? </p>
<ul>
<li>It reduce the number of input nodes (intput variables for NN) The mechanism of applying the convolutional kernal can be viewed as a kind of downsampling process.</li>
<li>Tolerating the small shift of the image</li>
<li>Utilizing the correlation between neighboring pixels.</li>
</ul>
<p>In CNN, a filter is a small square, such as image with 3 by 3 pixels. Before traning a CNN, we start with a random pixel values in kernel. End up traning with the backward proporgation, we end up with sth that is more meaningful.</p>
<p>Mathmatically, we mainly care about the discrete convolutional operation in CNN, here are some good <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Convolution">animations</a> about it. This is <a target="_blank" rel="noopener" href="https://www.songho.ca/dsp/convolution/convolution2d_example.html">equation</a> about the 1d and 2d convolutional operaton. Be careful about the offset of multiplying two functions used to compute the convolutional operation. Some online tutorial just mention that we time the value in kernel with the value in original matrix corespondingly. Actually, according to this <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Y7TMwqAWEdo">video</a>, the kernel has been flipped between row and column before applying the kernel operator. This is a <a target="_blank" rel="noopener" href="https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d">good explanation</a> about the convolution in 2d and 1d</p>
<p>1d convolution operation with 2d convolution operation</p>
<p>Coming back to the CNN, there are multiple ways to applying the kernel to the original input data<br>There are some realted terms: if we apply the kernel start from each value of the original matrix and there is overlapping between the region area covered by each kernel, the output result is <strong>Feature map</strong>. </p>
<p>Otherwise, if there is no overlapping for the areas covered by each kernel, and the kernel is just selecting the element that has a best output value, this approach is called <strong>max pooling</strong> if the filter is just to do the average operation of the selected area in the featuer map, this operation is called <strong>average pooling</strong>.</p>
<p>Ensentially, the pooling strategy is used to decrease the size of the feature map (otherwise, there are too many parameters), it is just a fancy term for downsamping or lossy compression. When using the kernel size for pooling as the <code>2*2</code>, the result is the 1&#x2F;4 of origianl size. </p>
<p>The number of kernel does not change the size of the image, it just changes the number of ouput channles. The number of pooling operation changes the size of the figure, for the <code>2*2</code> pooling kernel with stride equals 2, the size of the image becomes half of the original image. For example, in vgg, if the input size of the figure is 224, there are five pooling operation, the final image size connected to the fully connected layer is <code>224/(2^5) = 7</code></p>
<p>This <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=HGwBXDKFk9I&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=14">video</a> provides an exmaple that is easy to understand about the CNN.</p>
<p><strong>how to use the torch nn module</strong></p>
<p>Looking at this vedio<br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=n8Mey4o8gLc">https://www.youtube.com/watch?v=n8Mey4o8gLc</a></p>
<p>one input matrix, one output</p>
<p>one input image (three channels), one output channel<br>Assuming there is <code>3*5*5</code> input (3 channel with 5 pixel hight and 5 pixel width), we apply a <code>3*3</code> kernel for it, then we can get the <code>3*3*3</code> smaller feature map. But the expected output is one channel, so we sum the each coresponding pixel values of these three channels together. The result is <code>1*3*3</code>.</p>
<p>one input image (three channels), two output channels<br>This video provides a good explanation(<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=KTB_OFoAQcc">https://www.youtube.com/watch?v=KTB_OFoAQcc</a>)<br>Ensentially, we could use multiple filters (filter kernel), the number of output channel equals to the number of filter we used for extracting features.</p>
<h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1K94y1Z7wn?p=23&amp;vd_source=84e49edbada3d8cd23c5aa1d7957d087">https://www.bilibili.com/video/BV1K94y1Z7wn?p=23&amp;vd_source=84e49edbada3d8cd23c5aa1d7957d087</a></p>
<p>This video provides a good explanaiton for understanding things.</p>
<p>Important idea: increasing the number of the feature map when there is loss of information during the maxpooling. 用特征图的个数的增加弥补pooling所带来的损失。</p>
<p>随着层数的增加，feature的质量会降低。</p>
<p>nn.dropout<br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=NhZVe50QwPM">https://www.youtube.com/watch?v=NhZVe50QwPM</a><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=gxrnkqa9amo">https://www.youtube.com/watch?v=gxrnkqa9amo</a><br>there is a probability to set the activation between two layers as zero<br>with some probability, we remove some neural (we get a new network with less neural)<br>(help to remove the overfitting)</p>
<p>nn.sequential, if the module is sequentially called, just use the sequential to wrap these layers. We only need on forward operation for all of the chained model wrapped by sequential. </p>
<p>nn.linear, just the fully connected models between two layers.</p>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><h3 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h3><p>how to load other existing model and then adjust their parameters in small scale to make them fit your different task.</p>
<h3 id="RNN-and-LSTM"><a href="#RNN-and-LSTM" class="headerlink" title="RNN and LSTM"></a>RNN and LSTM</h3><h3 id="Siamese-Network"><a href="#Siamese-Network" class="headerlink" title="Siamese Network"></a>Siamese Network</h3><p>This is a good reference<br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=6jfw8MuKwpI">https://www.youtube.com/watch?v=6jfw8MuKwpI</a></p>
<p>The core idea is the distance between two inputs</p>
<h3 id="Other-need-to-be-studied"><a href="#Other-need-to-be-studied" class="headerlink" title="Other need to be studied"></a>Other need to be studied</h3><p>ResNet,<br>Seq2Seq,</p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h3 id="Vision-Transformer-vs-CNN"><a href="#Vision-Transformer-vs-CNN" class="headerlink" title="Vision Transformer vs CNN"></a>Vision Transformer vs CNN</h3><p>Is it more efficient to use Transformer compared with the CNN</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>Good online cources on bilibili</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1zF411V7xu/?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV1zF411V7xu/?spm_id_from=333.337.search-card.all.click</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2023/07/09/deep-leaning-tips/" data-id="cm2a0xf4n0076dvjr9hkq9kxe" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
    
 
<script src="/jquery/jquery.min.js"></script>

  <div id="random_posts">
    <h2>推荐文章</h2>
    <div class="random_posts_ul">
      <script>
          var random_count =4
          var site = {BASE_URI:'/'};
          function load_random_posts(obj) {
              var arr=site.posts;
              if (!obj) return;
              // var count = $(obj).attr('data-count') || 6;
              for (var i, tmp, n = arr.length; n; i = Math.floor(Math.random() * n), tmp = arr[--n], arr[n] = arr[i], arr[i] = tmp);
              arr = arr.slice(0, random_count);
              var html = '<ul>';
            
              for(var j=0;j<arr.length;j++){
                var item=arr[j];
                html += '<li><strong>' + 
                item.date + ':&nbsp;&nbsp;<a href="' + (site.BASE_URI+item.uri) + '">' + 
                (item.title || item.uri) + '</a></strong>';
                if(item.excerpt){
                  html +='<div class="post-excerpt">'+item.excerpt+'</div>';
                }
                html +='</li>';
                
              }
              $(obj).html(html + '</ul>');
          }
          $('.random_posts_ul').each(function () {
              var c = this;
              if (!site.posts || !site.posts.length){
                  $.getJSON(site.BASE_URI + 'js/posts.js',function(json){site.posts = json;load_random_posts(c)});
              } 
               else{
                load_random_posts(c);
              }
          });
      </script>
    </div>
  </div>

    
<nav id="article-nav">
  
    <a href="/2023/07/15/statistics-review/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          Statistics Review
        
      </div>
    </a>
  
  
    <a href="/2023/05/28/Many-aspects-of-cpp-template/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">Many aspects of cpp template</div>
    </a>
  
</nav>

  
</article>
 
     
  <div class="comments" id="comments">
    
     
       
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
       
      
      
  </div>
 
  

</section>
           
    <aside id="sidebar">
  
    
    <div class="widget-wrap">
    
      <div class="widget" id="toc-widget-fixed">
      
        <strong class="toc-title">文章目录</strong>
        <div class="toc-widget-list">
              <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Things-need-to-consider-before-using-NN"><span class="toc-number">1.</span> <span class="toc-text">Things need to consider before using NN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Types-of-the-problem"><span class="toc-number">2.</span> <span class="toc-text">Types of the problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Format-of-the-data"><span class="toc-number">3.</span> <span class="toc-text">Format of the data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-transformation-and-non-linear-transformation"><span class="toc-number">4.</span> <span class="toc-text">Linear transformation and non-linear transformation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Typical-NN"><span class="toc-number">5.</span> <span class="toc-text">Typical NN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-descent-appraoch"><span class="toc-number">5.1.</span> <span class="toc-text">Gradient descent appraoch</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Backward-propagation"><span class="toc-number">5.2.</span> <span class="toc-text">Backward propagation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-defination-about-epoch-and-batch"><span class="toc-number">5.3.</span> <span class="toc-text">The defination about epoch and batch</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-dynamic-programming-to-find-the-gradient"><span class="toc-number">5.4.</span> <span class="toc-text">Using dynamic programming to find the gradient</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Activation-function"><span class="toc-number">5.5.</span> <span class="toc-text">Activation function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#An-simple-pytorch-exmaple"><span class="toc-number">5.6.</span> <span class="toc-text">An simple pytorch exmaple</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ArgMax-and-SoftMax"><span class="toc-number">5.7.</span> <span class="toc-text">ArgMax and SoftMax</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-decent"><span class="toc-number">5.8.</span> <span class="toc-text">Gradient decent</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN"><span class="toc-number">6.</span> <span class="toc-text">CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VGG"><span class="toc-number">7.</span> <span class="toc-text">VGG</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet"><span class="toc-number">8.</span> <span class="toc-text">ResNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transfer-learning"><span class="toc-number">9.</span> <span class="toc-text">Transfer learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-and-LSTM"><span class="toc-number">10.</span> <span class="toc-text">RNN and LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Siamese-Network"><span class="toc-number">11.</span> <span class="toc-text">Siamese Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Other-need-to-be-studied"><span class="toc-number">12.</span> <span class="toc-text">Other need to be studied</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer"><span class="toc-number">13.</span> <span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Vision-Transformer-vs-CNN"><span class="toc-number">14.</span> <span class="toc-text">Vision Transformer vs CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#References"><span class="toc-number">15.</span> <span class="toc-text">References</span></a></li></ol>
          </div>
      </div>
    </div>

  
    

  
    
  
    
  
    

  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2024 zhe&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;godenwangzhe@gmail.com
    </div>
  </div>
</footer>
 
<script src="/jquery/jquery.min.js"></script>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  



 
<script src="/js/is.js"></script>



  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


<script src="/js/elevator.js"></script>

  </div>
</body>
</html>